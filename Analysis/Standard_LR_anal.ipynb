{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTNO</th>\n",
       "      <th>GNO</th>\n",
       "      <th>CBSCORE</th>\n",
       "      <th>CBSCOREGRD</th>\n",
       "      <th>CREDITOTAMT</th>\n",
       "      <th>YSALEAMT</th>\n",
       "      <th>ESTMM</th>\n",
       "      <th>ASSETAMT</th>\n",
       "      <th>IMSAAMT</th>\n",
       "      <th>IMJUAMT</th>\n",
       "      <th>...</th>\n",
       "      <th>환율</th>\n",
       "      <th>GDP</th>\n",
       "      <th>소상공인체감지수</th>\n",
       "      <th>실업률</th>\n",
       "      <th>물가지수</th>\n",
       "      <th>국고채</th>\n",
       "      <th>금리</th>\n",
       "      <th>유가등락률</th>\n",
       "      <th>소비자심리지수</th>\n",
       "      <th>ONEHOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475821</td>\n",
       "      <td>l180202101898</td>\n",
       "      <td>1.483693</td>\n",
       "      <td>-1.008776</td>\n",
       "      <td>-0.182324</td>\n",
       "      <td>5.599410</td>\n",
       "      <td>-0.351065</td>\n",
       "      <td>-0.497246</td>\n",
       "      <td>-0.176818</td>\n",
       "      <td>-0.327516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068090</td>\n",
       "      <td>2.274533</td>\n",
       "      <td>-1.302456</td>\n",
       "      <td>-0.290076</td>\n",
       "      <td>2.728294</td>\n",
       "      <td>-1.492204</td>\n",
       "      <td>-3.801670</td>\n",
       "      <td>2.712699</td>\n",
       "      <td>1.300530</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74417</td>\n",
       "      <td>l230201700120</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>-0.269293</td>\n",
       "      <td>-0.187359</td>\n",
       "      <td>3.004204</td>\n",
       "      <td>-0.774902</td>\n",
       "      <td>-0.434147</td>\n",
       "      <td>-0.395499</td>\n",
       "      <td>0.047573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509001</td>\n",
       "      <td>-1.236236</td>\n",
       "      <td>-1.058419</td>\n",
       "      <td>0.571922</td>\n",
       "      <td>-0.733929</td>\n",
       "      <td>-0.363940</td>\n",
       "      <td>-0.649856</td>\n",
       "      <td>1.705658</td>\n",
       "      <td>-1.167094</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>387787</td>\n",
       "      <td>l110201603233</td>\n",
       "      <td>0.682990</td>\n",
       "      <td>-1.008776</td>\n",
       "      <td>-0.283032</td>\n",
       "      <td>1.765241</td>\n",
       "      <td>1.889216</td>\n",
       "      <td>-0.497246</td>\n",
       "      <td>-0.176818</td>\n",
       "      <td>-0.327516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991659</td>\n",
       "      <td>-1.589789</td>\n",
       "      <td>0.853200</td>\n",
       "      <td>-0.290076</td>\n",
       "      <td>-1.872513</td>\n",
       "      <td>-0.546902</td>\n",
       "      <td>-0.649856</td>\n",
       "      <td>0.170943</td>\n",
       "      <td>-0.965339</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>395418</td>\n",
       "      <td>l230201700254</td>\n",
       "      <td>0.169719</td>\n",
       "      <td>-0.269293</td>\n",
       "      <td>-0.212536</td>\n",
       "      <td>1.647960</td>\n",
       "      <td>-0.744628</td>\n",
       "      <td>-0.497246</td>\n",
       "      <td>-0.176818</td>\n",
       "      <td>-0.327516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490639</td>\n",
       "      <td>-1.236236</td>\n",
       "      <td>1.219254</td>\n",
       "      <td>-0.721075</td>\n",
       "      <td>-0.788147</td>\n",
       "      <td>-0.241965</td>\n",
       "      <td>-0.649856</td>\n",
       "      <td>0.665173</td>\n",
       "      <td>-0.810142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190372</td>\n",
       "      <td>l200201601418</td>\n",
       "      <td>-3.033092</td>\n",
       "      <td>2.688638</td>\n",
       "      <td>-0.247784</td>\n",
       "      <td>0.478163</td>\n",
       "      <td>1.616749</td>\n",
       "      <td>-0.202785</td>\n",
       "      <td>-0.351763</td>\n",
       "      <td>-0.327516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409321</td>\n",
       "      <td>-1.809655</td>\n",
       "      <td>-0.102610</td>\n",
       "      <td>-0.721075</td>\n",
       "      <td>-2.530877</td>\n",
       "      <td>-1.736154</td>\n",
       "      <td>-0.649856</td>\n",
       "      <td>-0.958727</td>\n",
       "      <td>-0.142797</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTNO            GNO   CBSCORE  CBSCOREGRD  CREDITOTAMT  YSALEAMT  \\\n",
       "0  475821  l180202101898  1.483693   -1.008776    -0.182324  5.599410   \n",
       "1   74417  l230201700120  0.436620   -0.269293    -0.187359  3.004204   \n",
       "2  387787  l110201603233  0.682990   -1.008776    -0.283032  1.765241   \n",
       "3  395418  l230201700254  0.169719   -0.269293    -0.212536  1.647960   \n",
       "4  190372  l200201601418 -3.033092    2.688638    -0.247784  0.478163   \n",
       "\n",
       "      ESTMM  ASSETAMT   IMSAAMT   IMJUAMT  ...        환율       GDP  소상공인체감지수  \\\n",
       "0 -0.351065 -0.497246 -0.176818 -0.327516  ... -0.068090  2.274533 -1.302456   \n",
       "1 -0.774902 -0.434147 -0.395499  0.047573  ...  0.509001 -1.236236 -1.058419   \n",
       "2  1.889216 -0.497246 -0.176818 -0.327516  ...  0.991659 -1.589789  0.853200   \n",
       "3 -0.744628 -0.497246 -0.176818 -0.327516  ...  0.490639 -1.236236  1.219254   \n",
       "4  1.616749 -0.202785 -0.351763 -0.327516  ...  0.409321 -1.809655 -0.102610   \n",
       "\n",
       "        실업률      물가지수       국고채        금리     유가등락률   소비자심리지수  ONEHOT  \n",
       "0 -0.290076  2.728294 -1.492204 -3.801670  2.712699  1.300530       3  \n",
       "1  0.571922 -0.733929 -0.363940 -0.649856  1.705658 -1.167094       8  \n",
       "2 -0.290076 -1.872513 -0.546902 -0.649856  0.170943 -0.965339       7  \n",
       "3 -0.721075 -0.788147 -0.241965 -0.649856  0.665173 -0.810142       3  \n",
       "4 -0.721075 -2.530877 -1.736154 -0.649856 -0.958727 -0.142797       7  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = pd.read_csv(\"DATA/Standard_Scaler_result.csv\",encoding=\"cp949\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>ACTCD</td>      <th>  R-squared:         </th> <td>   0.061</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.060</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   168.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 21 Dec 2021</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:41:46</td>     <th>  Log-Likelihood:    </th> <td> -5568.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 39117</td>      <th>  AIC:               </th> <td>1.117e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 39101</td>      <th>  BIC:               </th> <td>1.131e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    15</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>   <td>    0.1075</td> <td>    0.004</td> <td>   26.406</td> <td> 0.000</td> <td>    0.100</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CBSCORE</th>     <td>   -0.0480</td> <td>    0.002</td> <td>  -29.323</td> <td> 0.000</td> <td>   -0.051</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CREDITOTAMT</th> <td>    0.0058</td> <td>    0.001</td> <td>    3.881</td> <td> 0.000</td> <td>    0.003</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>YSALEAMT</th>    <td>    0.0083</td> <td>    0.002</td> <td>    5.256</td> <td> 0.000</td> <td>    0.005</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ESTMM</th>       <td>   -0.0228</td> <td>    0.002</td> <td>  -15.021</td> <td> 0.000</td> <td>   -0.026</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IMSAAMT</th>     <td>   -0.0028</td> <td>    0.001</td> <td>   -1.921</td> <td> 0.055</td> <td>   -0.006</td> <td> 5.63e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IMJUAMT</th>     <td>   -0.0086</td> <td>    0.001</td> <td>   -5.954</td> <td> 0.000</td> <td>   -0.011</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BUJUAMT</th>     <td>   -0.0153</td> <td>    0.001</td> <td>  -10.416</td> <td> 0.000</td> <td>   -0.018</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BU1TOTAMT</th>   <td>    0.0048</td> <td>    0.001</td> <td>    3.207</td> <td> 0.001</td> <td>    0.002</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GAMT</th>        <td>   -0.0072</td> <td>    0.002</td> <td>   -3.971</td> <td> 0.000</td> <td>   -0.011</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GKIND</th>       <td>    0.0046</td> <td>    0.002</td> <td>    2.757</td> <td> 0.006</td> <td>    0.001</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>환율</th>          <td>    0.0059</td> <td>    0.002</td> <td>    2.544</td> <td> 0.011</td> <td>    0.001</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GDP</th>         <td>   -0.0253</td> <td>    0.001</td> <td>  -16.951</td> <td> 0.000</td> <td>   -0.028</td> <td>   -0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>국고채</th>         <td>    0.0097</td> <td>    0.002</td> <td>    3.973</td> <td> 0.000</td> <td>    0.005</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>유가등락률</th>       <td>    0.0046</td> <td>    0.002</td> <td>    2.820</td> <td> 0.005</td> <td>    0.001</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ONEHOT</th>      <td>   -0.0025</td> <td>    0.000</td> <td>   -7.785</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>19693.786</td> <th>  Durbin-Watson:     </th> <td>   1.719</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>90089.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 2.581</td>   <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 8.351</td>   <th>  Cond. No.          </th> <td>    31.5</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  ACTCD   R-squared:                       0.061\n",
       "Model:                            OLS   Adj. R-squared:                  0.060\n",
       "Method:                 Least Squares   F-statistic:                     168.1\n",
       "Date:                Tue, 21 Dec 2021   Prob (F-statistic):               0.00\n",
       "Time:                        17:41:46   Log-Likelihood:                -5568.3\n",
       "No. Observations:               39117   AIC:                         1.117e+04\n",
       "Df Residuals:                   39101   BIC:                         1.131e+04\n",
       "Df Model:                          15                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "intercept       0.1075      0.004     26.406      0.000       0.100       0.115\n",
       "CBSCORE        -0.0480      0.002    -29.323      0.000      -0.051      -0.045\n",
       "CREDITOTAMT     0.0058      0.001      3.881      0.000       0.003       0.009\n",
       "YSALEAMT        0.0083      0.002      5.256      0.000       0.005       0.011\n",
       "ESTMM          -0.0228      0.002    -15.021      0.000      -0.026      -0.020\n",
       "IMSAAMT        -0.0028      0.001     -1.921      0.055      -0.006    5.63e-05\n",
       "IMJUAMT        -0.0086      0.001     -5.954      0.000      -0.011      -0.006\n",
       "BUJUAMT        -0.0153      0.001    -10.416      0.000      -0.018      -0.012\n",
       "BU1TOTAMT       0.0048      0.001      3.207      0.001       0.002       0.008\n",
       "GAMT           -0.0072      0.002     -3.971      0.000      -0.011      -0.004\n",
       "GKIND           0.0046      0.002      2.757      0.006       0.001       0.008\n",
       "환율              0.0059      0.002      2.544      0.011       0.001       0.010\n",
       "GDP            -0.0253      0.001    -16.951      0.000      -0.028      -0.022\n",
       "국고채             0.0097      0.002      3.973      0.000       0.005       0.014\n",
       "유가등락률           0.0046      0.002      2.820      0.005       0.001       0.008\n",
       "ONEHOT         -0.0025      0.000     -7.785      0.000      -0.003      -0.002\n",
       "==============================================================================\n",
       "Omnibus:                    19693.786   Durbin-Watson:                   1.719\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            90089.052\n",
       "Skew:                           2.581   Prob(JB):                         0.00\n",
       "Kurtosis:                       8.351   Cond. No.                         31.5\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['intercept'] = 1\n",
    "\n",
    "lm = sm.OLS(data['ACTCD'], data[[ 'intercept', 'CBSCORE', 'CREDITOTAMT', 'YSALEAMT',\n",
    "       'ESTMM', 'IMSAAMT', 'IMJUAMT',  'BUJUAMT',\n",
    "       'BU1TOTAMT', 'GAMT', 'GKIND',  '환율', 'GDP', \n",
    "        '국고채', '유가등락률', 'ONEHOT']]) \n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['CBSCORE', 'CREDITOTAMT', 'YSALEAMT',\n",
    "       'ESTMM',  'IMSAAMT', 'IMJUAMT', 'BUSAAMT', 'BUJUAMT',\n",
    "       'BU1TOTAMT', 'GAMT', 'LABORCNT', 'KOSPI', '환율', 'GDP', \n",
    "       '소상공인체감지수', '실업률', '물가지수', '국고채', '금리', '유가등락률',\n",
    "       '소비자심리지수','ONEHOT']]\n",
    "act = data['ACTCD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = act - 1\n",
    "act = act * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27381, 22) (11736, 22) (27381,) (11736,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x,train_y, test_y = train_test_split(features, act,\n",
    "stratify=act,train_size=0.7,test_size=0.3,random_state=1)\n",
    "\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE, ADASYN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "#언더 샘플링\n",
    "enn = EditedNearestNeighbours(kind_sel=\"all\", n_neighbors=10)\n",
    "tomekl = TomekLinks()\n",
    "\n",
    "#오버 샘플링\n",
    "bsmote = BorderlineSMOTE(random_state=42)\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "#혼합 샘플링\n",
    "smotee = SMOTEENN(random_state=42)\n",
    "smoteT = SMOTETomek(random_state=42)\n",
    "\n",
    "\n",
    "X_under1_train, Y_under1_train = enn.fit_resample(train_x, train_y)\n",
    "X_under2_train, Y_under2_train = tomekl.fit_resample(train_x, train_y)\n",
    "\n",
    "X_over1_train, Y_over1_train = bsmote.fit_resample(train_x,train_y)\n",
    "X_over2_train, Y_over2_train = adasyn.fit_resample(train_x,train_y)\n",
    "\n",
    "X_comb1_train, Y_comb1_train = smotee.fit_resample(train_x, train_y)\n",
    "X_comb2_train, Y_comb2_train = smoteT.fit_resample(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래 분석 결과, Logistic Regression 하이퍼 파라미터: C와 max_iter 조정 결과 변화 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임계값 함수\n",
    "\n",
    "def cut_off(y, threshold) :\n",
    "    Y = y.copy()  # 대문자 Y를 새로운 변수로 하여 기존의 y값에 영향이 가지 않도록 한다.\n",
    "    Y[Y>threshold] = 1\n",
    "    Y[Y<threshold] = 0\n",
    "    return Y.astype(int)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# confusion matrix accuracy(정확도) 계산함수\n",
    "\n",
    "def acc(cfmat):\n",
    "    return (cfmat[0,0] + cfmat[1,1])/(cfmat[0,0] + cfmat[1,1] + cfmat[0,1] + cfmat[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579274\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579274\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579274\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579274\n",
      "         Iterations 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score\n",
    "\n",
    "anal_result = []\n",
    "m = [1000, 1500, 2000, 2500, 3000]\n",
    "c = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for i in m:\n",
    "    model = LogisticRegression(random_state = 42, solver = 'lbfgs', max_iter = i)\n",
    "    results = model.fit(X_over1_train,Y_over1_train)\n",
    "    \n",
    "    #로지스틱 모형 적합\n",
    "    model = sm.Logit(Y_over1_train,X_over1_train)\n",
    "    results = model.fit()\n",
    "\n",
    "    pred_y = results.predict(test_x)\n",
    "    pred_Y = cut_off(pred_y, 0.5)\n",
    "\n",
    "    cfmat = confusion_matrix(test_y,pred_Y)\n",
    "    accuracy = acc(cfmat)\n",
    "\n",
    "    pr = precision_score(test_y,pred_Y)\n",
    "    r = recall_score(test_y,pred_Y)\n",
    "    f1= f1_score(test_y,pred_Y)\n",
    "    auc_score = roc_auc_score(test_y,pred_y)\n",
    "    \n",
    "    anal_result.append((i, accuracy, cfmat, pr, r, f1, auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.550403\n",
      "         Iterations 6\n",
      "[[ 722  348]\n",
      " [3242 7424]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        사고보증       0.18      0.67      0.29      1070\n",
      "        정상보증       0.96      0.70      0.81     10666\n",
      "\n",
      "    accuracy                           0.69     11736\n",
      "   macro avg       0.57      0.69      0.55     11736\n",
      "weighted avg       0.88      0.69      0.76     11736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = model.fit(X_over1_train,Y_over1_train)\n",
    "model = sm.Logit(Y_over1_train,X_over1_train)\n",
    "results = model.fit()\n",
    "pred_y = results.predict(test_x)\n",
    "pred_Y = cut_off(pred_y, 0.5)\n",
    "cfmat = confusion_matrix(test_y,pred_Y)\n",
    "print(cfmat)\n",
    "print(classification_report(test_y, pred_Y, target_names=['사고보증', '정상보증']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1500, 0.5389400136332652, array([[5446, 5220],\n",
      "       [ 191,  879]], dtype=int64), 0.1441219872110182, 0.8214953271028037, 0.24522248570232946, 0.7491535247822146), (2000, 0.5389400136332652, array([[5446, 5220],\n",
      "       [ 191,  879]], dtype=int64), 0.1441219872110182, 0.8214953271028037, 0.24522248570232946, 0.7491535247822146), (2500, 0.5389400136332652, array([[5446, 5220],\n",
      "       [ 191,  879]], dtype=int64), 0.1441219872110182, 0.8214953271028037, 0.24522248570232946, 0.7491535247822146), (3000, 0.5389400136332652, array([[5446, 5220],\n",
      "       [ 191,  879]], dtype=int64), 0.1441219872110182, 0.8214953271028037, 0.24522248570232946, 0.7491535247822146)]\n"
     ]
    }
   ],
   "source": [
    "print(anal_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.611243\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.611243\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.611243\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.611243\n",
      "         Iterations 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score\n",
    "\n",
    "anal_result = []\n",
    "m = [1000, 1500, 2000, 2500, 3000]\n",
    "c = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for i in m:\n",
    "    model = LogisticRegression(random_state = 42, solver = 'lbfgs', max_iter = 1000, C = i)\n",
    "    results = model.fit(X_over2_train,Y_over2_train)\n",
    "    \n",
    "    #로지스틱 모형 적합\n",
    "    model = sm.Logit(Y_over2_train,X_over2_train)\n",
    "    results = model.fit()\n",
    "\n",
    "    pred_y = results.predict(test_x)\n",
    "    pred_Y = cut_off(pred_y, 0.5)\n",
    "\n",
    "    cfmat = confusion_matrix(test_y,pred_Y)\n",
    "    accuracy = acc(cfmat)\n",
    "\n",
    "    pr = precision_score(test_y,pred_Y)\n",
    "    r = recall_score(test_y,pred_Y)\n",
    "    f1= f1_score(test_y,pred_Y)\n",
    "    auc_score = roc_auc_score(test_y,pred_y)\n",
    "    \n",
    "    anal_result.append((i, accuracy, cfmat, pr, r, f1, auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.595659\n",
      "         Iterations 6\n",
      "[[ 779  291]\n",
      " [3647 7019]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        사고보증       0.18      0.73      0.28      1070\n",
      "        정상보증       0.96      0.66      0.78     10666\n",
      "\n",
      "    accuracy                           0.66     11736\n",
      "   macro avg       0.57      0.69      0.53     11736\n",
      "weighted avg       0.89      0.66      0.74     11736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "results = model.fit(X_over2_train,Y_over2_train)\n",
    "model = sm.Logit(Y_over2_train,X_over2_train)\n",
    "results = model.fit()\n",
    "pred_y = results.predict(test_x)\n",
    "pred_Y = cut_off(pred_y, 0.5)\n",
    "cfmat = confusion_matrix(test_y,pred_Y)\n",
    "print(cfmat)\n",
    "print(classification_report(test_y, pred_Y, target_names=['사고보증', '정상보증']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1500, 0.5328902522154055, array([[5363, 5303],\n",
      "       [ 179,  891]], dtype=int64), 0.1438488860187278, 0.8327102803738318, 0.2453193832599119, 0.7484908373362121), (2000, 0.5328902522154055, array([[5363, 5303],\n",
      "       [ 179,  891]], dtype=int64), 0.1438488860187278, 0.8327102803738318, 0.2453193832599119, 0.7484908373362121), (2500, 0.5328902522154055, array([[5363, 5303],\n",
      "       [ 179,  891]], dtype=int64), 0.1438488860187278, 0.8327102803738318, 0.2453193832599119, 0.7484908373362121), (3000, 0.5328902522154055, array([[5363, 5303],\n",
      "       [ 179,  891]], dtype=int64), 0.1438488860187278, 0.8327102803738318, 0.2453193832599119, 0.7484908373362121)]\n"
     ]
    }
   ],
   "source": [
    "print(anal_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언더샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision :  0.1438488860187278\n",
      "recall :  0.8327102803738318\n",
      "f1_score: 0.2453193832599119\n",
      "auc_score :  0.7484908373362121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score\n",
    "\n",
    "pr = precision_score(test_y,pred_Y)\n",
    "print(\"precision : \",pr)\n",
    "r = recall_score(test_y,pred_Y)\n",
    "print(\"recall : \",r)\n",
    "f1= f1_score(test_y,pred_Y)\n",
    "print(\"f1_score:\",f1)\n",
    "auc_score = roc_auc_score(test_y,pred_y)\n",
    "print(\"auc_score : \",auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35620    0.492274\n",
      "13304    0.524007\n",
      "38803    0.607100\n",
      "8618     0.514990\n",
      "36794    0.625699\n",
      "           ...   \n",
      "4064     0.565968\n",
      "7401     0.349983\n",
      "28336    0.375821\n",
      "34226    0.419129\n",
      "22837    0.368435\n",
      "Length: 11736, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score\n",
    "\n",
    "anal_result = []\n",
    "m = [1000, 1500, 2000, 2500, 3000]\n",
    "c = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for i in m:\n",
    "    model = LogisticRegression(random_state = 42, solver = 'lbfgs', max_iter = i)\n",
    "    results = model.fit(X_under1_train,Y_under1_train)\n",
    "    \n",
    "    #로지스틱 모형 적합\n",
    "    model = sm.Logit(Y_under1_train,X_under1_train)\n",
    "    results = model.fit()\n",
    "\n",
    "    pred_y = results.predict(test_x)\n",
    "    pred_Y = cut_off(pred_y, 0.5)\n",
    "\n",
    "    cfmat = confusion_matrix(test_y,pred_Y)\n",
    "    accuracy = acc(cfmat)\n",
    "\n",
    "    pr = precision_score(test_y,pred_Y)\n",
    "    r = recall_score(test_y,pred_Y)\n",
    "    f1= f1_score(test_y,pred_Y)\n",
    "    auc_score = roc_auc_score(test_y,pred_y)\n",
    "    \n",
    "    anal_result.append((i, accuracy, cfmat, pr, r, f1, auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.331657\n",
      "         Iterations 8\n",
      "[[ 460  610]\n",
      " [1596 9070]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        사고보증       0.22      0.43      0.29      1070\n",
      "        정상보증       0.94      0.85      0.89     10666\n",
      "\n",
      "    accuracy                           0.81     11736\n",
      "   macro avg       0.58      0.64      0.59     11736\n",
      "weighted avg       0.87      0.81      0.84     11736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "results = model.fit(X_under1_train,Y_under1_train)\n",
    "model = sm.Logit(Y_under1_train,X_under1_train)\n",
    "results = model.fit()\n",
    "pred_y = results.predict(test_x)\n",
    "pred_Y = cut_off(pred_y, 0.5)\n",
    "cfmat = confusion_matrix(test_y,pred_Y)\n",
    "print(cfmat)\n",
    "print(classification_report(test_y, pred_Y, target_names=['사고보증', '정상보증']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35620    0\n",
       "13304    1\n",
       "38803    1\n",
       "8618     1\n",
       "36794    1\n",
       "        ..\n",
       "4064     1\n",
       "7401     0\n",
       "28336    0\n",
       "34226    0\n",
       "22837    0\n",
       "Length: 11736, dtype: int32"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(anal_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. TomekLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score\n",
    "\n",
    "anal_result = []\n",
    "m = [1000, 1500, 2000, 2500, 3000]\n",
    "c = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for i in m:\n",
    "    model = LogisticRegression(random_state = 42, solver = 'lbfgs', max_iter = i)\n",
    "    results = model.fit(X_under2_train,Y_under2_train)\n",
    "    \n",
    "    #로지스틱 모형 적합\n",
    "    model = sm.Logit(Y_under2_train,X_under2_train)\n",
    "    results = model.fit()\n",
    "\n",
    "    pred_y = results.predict(test_x)\n",
    "    pred_Y = cut_off(pred_y, 0.5)\n",
    "\n",
    "    cfmat = confusion_matrix(test_y,pred_Y)\n",
    "    accuracy = acc(cfmat)\n",
    "\n",
    "    pr = precision_score(test_y,pred_Y)\n",
    "    r = recall_score(test_y,pred_Y)\n",
    "    f1= f1_score(test_y,pred_Y)\n",
    "    auc_score = roc_auc_score(test_y,pred_y)\n",
    "    \n",
    "    anal_result.append((i, accuracy, cfmat, pr, r, f1, auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.312741\n",
      "         Iterations 7\n",
      "[[   66  1004]\n",
      " [  155 10511]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        사고보증       0.30      0.06      0.10      1070\n",
      "        정상보증       0.91      0.99      0.95     10666\n",
      "\n",
      "    accuracy                           0.90     11736\n",
      "   macro avg       0.61      0.52      0.52     11736\n",
      "weighted avg       0.86      0.90      0.87     11736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "results = model.fit(X_under2_train,Y_under2_train)\n",
    "model = sm.Logit(Y_under2_train,X_under2_train)\n",
    "results = model.fit()\n",
    "pred_y = results.predict(test_x)\n",
    "pred_Y = cut_off(pred_y, 0.5)\n",
    "cfmat = confusion_matrix(test_y,pred_Y)\n",
    "print(cfmat)\n",
    "print(classification_report(test_y, pred_Y, target_names=['사고보증', '정상보증']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anal_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 혼합샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35620    0.562487\n",
      "13304    0.605198\n",
      "38803    0.894126\n",
      "8618     0.527672\n",
      "36794    0.884854\n",
      "           ...   \n",
      "4064     0.652166\n",
      "7401     0.043432\n",
      "28336    0.016551\n",
      "34226    0.215665\n",
      "22837    0.032817\n",
      "Length: 11736, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score\n",
    "\n",
    "anal_result = []\n",
    "m = [1000, 1500, 2000, 2500, 3000]\n",
    "c = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for i in m:\n",
    "    model = LogisticRegression(random_state = 42, solver = 'lbfgs', max_iter = i)\n",
    "    results = model.fit(X_comb1_train,Y_comb1_train)\n",
    "    \n",
    "    #로지스틱 모형 적합\n",
    "    model = sm.Logit(Y_comb1_train,X_comb1_train)\n",
    "    results = model.fit()\n",
    "\n",
    "    pred_y = results.predict(test_x)\n",
    "    pred_Y = cut_off(pred_y, 0.5)\n",
    "\n",
    "    cfmat = confusion_matrix(test_y,pred_Y)\n",
    "    accuracy = acc(cfmat)\n",
    "\n",
    "    pr = precision_score(test_y,pred_Y)\n",
    "    r = recall_score(test_y,pred_Y)\n",
    "    f1= f1_score(test_y,pred_Y)\n",
    "    auc_score = roc_auc_score(test_y,pred_y)\n",
    "    \n",
    "    anal_result.append((i, accuracy, cfmat, pr, r, f1, auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.503193\n",
      "         Iterations 7\n",
      "[[ 870  200]\n",
      " [4792 5874]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        사고보증       0.15      0.81      0.26      1070\n",
      "        정상보증       0.97      0.55      0.70     10666\n",
      "\n",
      "    accuracy                           0.57     11736\n",
      "   macro avg       0.56      0.68      0.48     11736\n",
      "weighted avg       0.89      0.57      0.66     11736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "results = model.fit(X_comb1_train,Y_comb1_train)\n",
    "model = sm.Logit(Y_comb1_train,X_comb1_train)\n",
    "results = model.fit()\n",
    "pred_y = results.predict(test_x)\n",
    "pred_Y = cut_off(pred_y, 0.5)\n",
    "cfmat = confusion_matrix(test_y,pred_Y)\n",
    "print(cfmat)\n",
    "print(classification_report(test_y, pred_Y, target_names=['사고보증', '정상보증']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35620    1\n",
       "13304    1\n",
       "38803    1\n",
       "8618     1\n",
       "36794    1\n",
       "        ..\n",
       "4064     1\n",
       "7401     0\n",
       "28336    0\n",
       "34226    0\n",
       "22837    0\n",
       "Length: 11736, dtype: int32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(anal_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.601275\n",
      "         Iterations 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score\n",
    "\n",
    "anal_result = []\n",
    "m = [1000, 1500, 2000, 2500, 3000]\n",
    "c = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for i in m:\n",
    "    model = LogisticRegression(random_state = 42, solver = 'lbfgs', max_iter = i)\n",
    "    results = model.fit(X_comb2_train,Y_comb2_train)\n",
    "    \n",
    "    #로지스틱 모형 적합\n",
    "    model = sm.Logit(Y_comb2_train,X_comb2_train)\n",
    "    results = model.fit()\n",
    "\n",
    "    pred_y = results.predict(test_x)\n",
    "    pred_Y = cut_off(pred_y, 0.5)\n",
    "\n",
    "    cfmat = confusion_matrix(test_y,pred_Y)\n",
    "    accuracy = acc(cfmat)\n",
    "\n",
    "    pr = precision_score(test_y,pred_Y)\n",
    "    r = recall_score(test_y,pred_Y)\n",
    "    f1= f1_score(test_y,pred_Y)\n",
    "    auc_score = roc_auc_score(test_y,pred_y)\n",
    "    \n",
    "    anal_result.append((i, accuracy, cfmat, pr, r, f1, auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.582487\n",
      "         Iterations 6\n",
      "[[ 767  303]\n",
      " [3559 7107]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        사고보증       0.18      0.72      0.28      1070\n",
      "        정상보증       0.96      0.67      0.79     10666\n",
      "\n",
      "    accuracy                           0.67     11736\n",
      "   macro avg       0.57      0.69      0.54     11736\n",
      "weighted avg       0.89      0.67      0.74     11736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "results = model.fit(X_comb2_train,Y_comb2_train)\n",
    "model = sm.Logit(Y_comb2_train,X_comb2_train)\n",
    "results = model.fit()\n",
    "pred_y = results.predict(test_x)\n",
    "pred_Y = cut_off(pred_y, 0.5)\n",
    "cfmat = confusion_matrix(test_y,pred_Y)\n",
    "print(cfmat)\n",
    "print(classification_report(test_y, pred_Y, target_names=['사고보증', '정상보증']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35620    1\n",
       "13304    1\n",
       "38803    1\n",
       "8618     1\n",
       "36794    1\n",
       "        ..\n",
       "4064     1\n",
       "7401     0\n",
       "28336    0\n",
       "34226    0\n",
       "22837    0\n",
       "Length: 11736, dtype: int32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(anal_result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4000436974652e2da39f7abf97d4cef3d01362244b176d22af40d9d73df68d1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Study_Big': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
